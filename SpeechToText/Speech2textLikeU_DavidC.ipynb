{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLu9phstnCAp"
      },
      "source": [
        "# LikeU Speech2text\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix4qPAq0BdjB"
      },
      "source": [
        "David Cardona Duque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL_d1hggnE6e"
      },
      "source": [
        "\n",
        "## Instalación de bibliotecas necesarias\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ufN7gwp34Pt"
      },
      "outputs": [],
      "source": [
        "pip install pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb9sdH5a35nS"
      },
      "outputs": [],
      "source": [
        "pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDu7GpDS36sV",
        "outputId": "4058ba01-6b4c-4c78-a427-c69ea0890092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DowAOGC38bQ"
      },
      "outputs": [],
      "source": [
        "pip install whisper-timestamped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyrRJW_LnLO-"
      },
      "source": [
        "\n",
        "## Importación de las bibliotecas necesarias\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q4wlWQvz4iWz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import datetime\n",
        "import whisper_timestamped as whisper\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1JB8NN4nRU8"
      },
      "source": [
        "\n",
        "## Función de transcripción y diarizacion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XwotHdrl4EME"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\" Convertir segundos a formato de tiempo HH:MM:SS \"\"\"\n",
        "    return str(datetime.timedelta(seconds=int(seconds)))\n",
        "\n",
        "def diarization(audio_file):\n",
        "      # Inicializar el modelo de diarización de pyannote\n",
        "    diarization_pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\",\n",
        "    use_auth_token=\"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\")\n",
        "    diarization_pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "    # Ejecutar diarización\n",
        "    diarization = diarization_pipeline(audio_file, num_speakers=2)\n",
        "\n",
        "    return diarization\n",
        "\n",
        "def transcribir_unificar(audio_file,model,nombre_archivo,diarization):\n",
        "\n",
        "\n",
        "    # Cargar y configurar el modelo de Whisper para el archivo completo\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    transcription_result = whisper.transcribe(model, audio, language=\"es\", vad=True,detect_disfluencies=True,\n",
        "                                               beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))\n",
        "    # Preparar el archivo de salida y procesar la diarización y transcripción\n",
        "    with open(f\"transcription{nombre_archivo}.txt\", \"w\", encoding='utf-8') as output_file:\n",
        "        last_speaker = None\n",
        "        last_start = 0\n",
        "        last_end = 0\n",
        "        last_transcription = \"\"\n",
        "        total_confidence = 0\n",
        "        segments = transcription_result['segments']\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            transcript_text = segment['text'].strip()\n",
        "            segment_confidence = segment['confidence']\n",
        "            total_confidence += segment_confidence\n",
        "\n",
        "            # Buscar el hablante que más se solapa con este segmento\n",
        "            speaker_label = None\n",
        "            max_overlap = 0\n",
        "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "                overlap = min(end_time, turn.end) - max(start_time, turn.start)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "                    speaker_label = speaker\n",
        "\n",
        "            # Si no hay hablante identificado y estamos en el primer o último segmento, asignamos el último o el primer hablante conocido\n",
        "            if not speaker_label:\n",
        "                if i == 0:\n",
        "                    speaker_label = last_speaker  # Use the last known speaker for the first segment if none identified\n",
        "                elif i == len(segments) - 1:\n",
        "                    speaker_label = last_speaker  # Use the last known speaker for the last segment if none identified\n",
        "\n",
        "            # Combinar segmentos si el hablante es el mismo y no hay pausa significativa\n",
        "            if speaker_label == last_speaker and (start_time - last_end) < 1:\n",
        "                last_end = end_time\n",
        "                last_transcription += \" \" + transcript_text\n",
        "            else:\n",
        "                if last_speaker is not None:\n",
        "                    # Escribir el segmento previo\n",
        "                    output_file.write(f\"{format_time(last_start)} - {format_time(last_end)} [{last_speaker}]: {last_transcription}\\n\")\n",
        "                last_speaker = speaker_label\n",
        "                last_start = start_time\n",
        "                last_end = end_time\n",
        "                last_transcription = transcript_text\n",
        "        # Calcular y mostrar la confianza promedio por segmento para toda la transcripción\n",
        "        average_confidence = total_confidence /  len(transcription_result['segments']) if transcription_result['segments'] else 0\n",
        "        print(f\"Diarization and transcription completed. Average segment confidence: {average_confidence:.2f}\")\n",
        "    return(average_confidence)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMtTutKWnsri"
      },
      "source": [
        "## Definicion del ciclo de uso diferenciado de modelo basado en la confianza promedio de los segmentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='Audios/1001764369_1.wav'>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transfrmación del audio wav49 a wav\n",
        "audio_path = \"Audios/1001764369_1.WAV\"\n",
        "wav49_audio = AudioSegment.from_file(audio_path, format=\"WAV\")\n",
        "wav49_audio.export(\"Audios/1001764369_1.wav\", format=\"wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aazodf_xigI"
      },
      "source": [
        "### Diarizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J5XB_puGxhXv"
      },
      "outputs": [],
      "source": [
        "audio_path = \"Audios/1001764369_1.WAV\"\n",
        "\n",
        "inicio = time.time()\n",
        "diarizacion=diarization(audio_path)\n",
        "fin = time.time()\n",
        "\n",
        "tiempo_ejecucion_diarizacion = fin - inicio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cjqct_2xj5Z"
      },
      "source": [
        "### Transcripcion y unificacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEkTWK2Hm9Qg",
        "outputId": "16c5ebe3-39fe-4897-bea2-da118b05c2d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [01:59<00:00, 4.04MiB/s]\n",
            "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\usuario/.cache\\torch\\hub\\master.zip\n",
            "100%|██████████| 55448/55448 [01:44<00:00, 530.59frames/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diarization and transcription completed. Average segment confidence: 0.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 55448/55448 [01:37<00:00, 569.07frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diarization and transcription completed. Average segment confidence: 0.39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 55448/55448 [07:40<00:00, 120.54frames/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudios/1001764369_1.WAV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m inicio \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 19\u001b[0m average_confidence\u001b[38;5;241m=\u001b[39m\u001b[43mtranscribir_unificar\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdiarizacion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m fin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     22\u001b[0m tiempo_ejecucion \u001b[38;5;241m=\u001b[39m fin \u001b[38;5;241m-\u001b[39m inicio\n",
            "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mtranscribir_unificar\u001b[1;34m(audio_file, model, nombre_archivo, diarization)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribir_unificar\u001b[39m(audio_file,model,nombre_archivo,diarization):\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Cargar y configurar el modelo de Whisper para el archivo completo\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     audio \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_audio(audio_file)\n\u001b[1;32m---> 23\u001b[0m     transcription_result \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdetect_disfluencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Preparar el archivo de salida y procesar la diarización y transcripción\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_archivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper_timestamped\\transcribe.py:290\u001b[0m, in \u001b[0;36mtranscribe_timestamped\u001b[1;34m(model, audio, language, task, remove_punctuation_from_words, compute_word_confidence, include_punctuation_in_confidence, refine_whisper_precision, min_word_duration, plot_word_alignment, word_alignement_most_top_layers, remove_empty_words, use_backend_timestamps, seed, vad, detect_disfluencies, trust_whisper_timestamps, naive_approach, temperature, best_of, beam_size, patience, length_penalty, compression_ratio_threshold, logprob_threshold, no_speech_threshold, fp16, condition_on_previous_text, initial_prompt, suppress_tokens, sample_len, verbose)\u001b[0m\n\u001b[0;32m    287\u001b[0m num_alignment_for_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m naive_approach:\n\u001b[1;32m--> 290\u001b[0m     (transcription, words) \u001b[38;5;241m=\u001b[39m \u001b[43m_transcribe_timestamped_naive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mmin_word_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Was 0.04 before 1.11\u001b[39;49;00m\n\u001b[0;32m    292\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtrust_whisper_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_whisper_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43muse_backend_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_backend_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43malignment_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwhisper_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     (transcription, words) \u001b[38;5;241m=\u001b[39m _transcribe_timestamped_efficient(model, audio,\n\u001b[0;32m    297\u001b[0m                                                                trust_whisper_timestamps\u001b[38;5;241m=\u001b[39mtrust_whisper_timestamps,\n\u001b[0;32m    298\u001b[0m                                                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39malignment_options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwhisper_options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_options)\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper_timestamped\\transcribe.py:1225\u001b[0m, in \u001b[0;36m_transcribe_timestamped_naive\u001b[1;34m(model, audio, remove_punctuation_from_words, compute_word_confidence, include_punctuation_in_confidence, refine_whisper_precision_nframes, use_backend_timestamps, alignment_heads, plot_word_alignment, word_alignement_most_top_layers, detect_disfluencies, trust_whisper_timestamps, min_word_duration, **whisper_options)\u001b[0m\n\u001b[0;32m   1222\u001b[0m i_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sot_sequence)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1225\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logprobs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1228\u001b[0m end_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtimestamp_begin \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mmin\u001b[39m(N_FRAMES \u001b[38;5;241m*\u001b[39m HOP_LENGTH, end_sample \u001b[38;5;241m-\u001b[39m start_sample) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m AUDIO_SAMPLES_PER_TOKEN)\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:265\u001b[0m, in \u001b[0;36mWhisper.forward\u001b[1;34m(self, mel, tokens)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m, mel: torch\u001b[38;5;241m.\u001b[39mTensor, tokens: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m    264\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tokens, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:170\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 170\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_post(x)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:139\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m    138\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln(x), xa, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 139\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:32\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2545\u001b[0m     )\n\u001b[1;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "modelos_disponibles=['small','medium','large-v3']\n",
        "\n",
        "confianzas=[]\n",
        "\n",
        "tiempos_ejecucion_transcripcion=[]\n",
        "\n",
        "average_confidence=0\n",
        "\n",
        "\n",
        "for modelo in modelos_disponibles:\n",
        "  if average_confidence <= 0.80:\n",
        "\n",
        "    model = whisper.load_model(modelo, device=device)\n",
        "\n",
        "    audio_path = \"Audios/1001764369_1.WAV\"\n",
        "    inicio = time.time()\n",
        "    average_confidence=transcribir_unificar(audio_path,model,modelo,diarizacion)\n",
        "    fin = time.time()\n",
        "\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    tiempos_ejecucion_transcripcion.append(tiempo_ejecucion)\n",
        "    confianzas.append(average_confidence)\n",
        "  else:\n",
        "      print(average_confidence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srB3I8OXtZ-B"
      },
      "source": [
        "### Analisis de confianza y tiempos de ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJrmFp0v2MO1",
        "outputId": "ba59178f-d0fd-4025-8c6b-20227b06ce1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.33244811320754725, 0.33660606060606046, 0.5392719999999999]\n"
          ]
        }
      ],
      "source": [
        "print(confianzas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt8GKN1B2Ol7",
        "outputId": "c05b4719-cc23-4dbc-b5f2-a95239aafc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[183.18469500541687, 411.6933948993683, 538.7805020809174]\n"
          ]
        }
      ],
      "source": [
        "print(tiempos_ejecucion_transcripcion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBuIMA1Z5gJa",
        "outputId": "a724cda5-ad5f-430d-b8a0-60e7aca78b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "244.7913806438446\n"
          ]
        }
      ],
      "source": [
        "print(tiempo_ejecucion_diarizacion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "tiempos_ejecucion_transcripcion = [183.18469500541687, 411.6933948993683, 538.7805020809174]\n",
        "confianza = [0.33, 0.34, 0.54]\n",
        "tiempodeejecuciondiarizacion = 244.7913806438446# Tiempo general de diarización en segundos\n",
        "modelos = ['Small', 'Medium', 'Largev3']  # Etiquetas para cada modelo\n",
        "\n",
        "# Crear figura de Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Añadir traza para los tiempos de ejecución de la transcripción\n",
        "fig.add_trace(go.Bar(\n",
        "    x=modelos,\n",
        "    y=tiempos_ejecucion_transcripcion,\n",
        "    name='Tiempo de Ejecución Transcripción',\n",
        "    marker_color='indianred',\n",
        "    yaxis='y'\n",
        "))\n",
        "\n",
        "# Añadir traza para la confianza\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=modelos,\n",
        "    y=confianza,\n",
        "    name='Confianza',\n",
        "    marker_color='blue',\n",
        "    yaxis='y2'\n",
        "))\n",
        "\n",
        "# Configuración de los ejes\n",
        "fig.update_layout(\n",
        "    title='Tiempo de Ejecución y Confianza por Modelo de Transcripción',\n",
        "    xaxis=dict(title='Modelo'),\n",
        "    yaxis=dict(title='Tiempo de Ejecución (s)', side='left', range=[0, max(tiempos_ejecucion_transcripcion) + 10]),\n",
        "    yaxis2=dict(title='Confianza', overlaying='y', side='right', range=[0.3, 0.6], tickformat=\".0%\"),\n",
        "    legend=dict(x=0.01, y=0.99, bordercolor='Black', borderwidth=1),\n",
        "    plot_bgcolor='floralwhite'\n",
        ")\n",
        "\n",
        "# Añadir una línea horizontal para el tiempo de diarización\n",
        "fig.add_hline(y=tiempodeejecuciondiarizacion, line_dash=\"dash\", annotation_text=\"Tiempo de Diarización General\",\n",
        "              annotation_position=\"bottom right\")\n",
        "\n",
        "# Mostrar figura\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8hsIIfCSCYz"
      },
      "source": [
        "### Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qKTSzKuRFYQ"
      },
      "source": [
        "\n",
        "Usar de manera directa el modelo large-v3 debido a que la confianza obtendia por los modelos Small y Medium en un audio de buena calidad es demasiado mala como para perder recursos y tiempo de ejecucion en correrlos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El2rXrmatfSr"
      },
      "source": [
        "## Analisis de conversion de formato"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBIIZHx00jvr"
      },
      "source": [
        "### Explicacion formatos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF_0DaMazzhK"
      },
      "source": [
        "**MP3 con libmp3lame (Bitrate: 128 kbps)**\n",
        "\n",
        "Formato y Codec: MP3 es uno de los formatos de audio más populares y comunes. Utiliza el codec libmp3lame, una implementación de alta calidad del estándar MP3.\n",
        "\n",
        "  - Pros: Altamente compatible con casi todos los dispositivos y plataformas de software. Buena reducción del tamaño del archivo con una pérdida de calidad relativamente baja a bitrates moderados o altos.\n",
        "  - Contras: Es un formato con pérdida, lo que significa que la calidad del audio se reduce al comprimir. A 128 kbps, la calidad es aceptable para la mayoría de los usos pero no es óptima para aplicaciones que requieren alta fidelidad.\n",
        "\n",
        "\n",
        "**FLAC**\n",
        "\n",
        "Formato y Codec: FLAC (Free Lossless Audio Codec) es un codec de compresión de audio sin pérdida, lo que significa que el audio se comprime sin perder ninguna información.\n",
        "\n",
        "  - Pros: Calidad de audio perfecta, sin pérdidas respecto al original. El tamaño del archivo es menor que el WAV sin compresión, aunque más grande que los formatos con pérdida.\n",
        "  - Contras: Los archivos son más grandes en comparación con MP3 o AAC a bitrates bajos o medios. No todos los dispositivos o sistemas lo soportan de manera nativa.\n",
        "\n",
        "**WAV con PCM**\n",
        "\n",
        "Formato y Codec: WAV es un formato de audio sin compresión que utiliza modulación por impulsos codificados (PCM) para representar el audio.\n",
        "\n",
        "  - Pros: Calidad de audio sin pérdidas, perfectamente fiel al original. Ideal para edición profesional de audio y para aplicaciones que requieren la máxima calidad.\n",
        "  - Contras: Tamaño de archivo muy grande, lo que lo hace impráctico para distribución o streaming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj44UyS00mzs"
      },
      "source": [
        "### Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HxrJmRMtidv"
      },
      "outputs": [],
      "source": [
        "conversiones = [\n",
        "    (\"output_mp3_128k.mp3\", \"libmp3lame\", \"128k\"),  # MP3 con bitrate de 128 kbps\n",
        "    (\"output_flac.flac\", \"flac\", None),             # FLAC sin pérdida\n",
        "    (\"output_wav_pcm.wav\", \"pcm_s16le\", None)       # WAV sin compresión\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1rWIYquzGsu",
        "outputId": "2b86d618-ea0a-4ba0-ba22-315999eda4e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo output_mp3_128k.mp3 creado.\n",
            "Archivo output_flac.flac creado.\n",
            "Archivo output_wav_pcm.wav creado.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "input_file = \"1001764369_1.WAV\"\n",
        "\n",
        "for output_file, codec, bitrate in conversiones:\n",
        "    if bitrate:\n",
        "        # Comando para conversiones con bitrate específico\n",
        "        command = f\"ffmpeg -i {input_file} -acodec {codec} -ar 44100 -b:a {bitrate} {output_file}\"\n",
        "    else:\n",
        "        # Comando para conversiones sin bitrate específico (sin pérdida)\n",
        "        command = f\"ffmpeg -i {input_file} -acodec {codec} -ar 44100 {output_file}\"\n",
        "    os.system(command)\n",
        "    print(f\"Archivo {output_file} creado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0siZJflQdQb"
      },
      "source": [
        "### Ejecucion por formato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03hZN4ET0q5E"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "audios_disponibles=['output_flac.flac','output_wav_pcm.wav','output_mp3_128k.mp3']\n",
        "\n",
        "confianzas=[]\n",
        "\n",
        "tiempos_ejecucion_transcripcion=[]\n",
        "tiempo_ejecucion_diarizacion=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = whisper.load_model('small', device=device)\n",
        "\n",
        "for audio in audios_disponibles:\n",
        "\n",
        "    inicio = time.time()\n",
        "    diarizacion=diarization(audio)\n",
        "    fin = time.time()\n",
        "\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    tiempo_ejecucion_diarizacion.append(tiempo_ejecucion)\n",
        "\n",
        "\n",
        "    inicio = time.time()\n",
        "    average_confidence=transcribir_unificar(audio,model,audio,diarizacion)\n",
        "    fin = time.time()\n",
        "\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    tiempos_ejecucion_transcripcion.append(tiempo_ejecucion)\n",
        "    confianzas.append(average_confidence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uAf1RzsQWQq"
      },
      "source": [
        "### Analisis por *formato*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpOwCXLO_H10",
        "outputId": "aea85644-885e-43e2-d544-2d4ed71e1f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[233.2945876121521, 63.3612494468689]\n"
          ]
        }
      ],
      "source": [
        "print(tiempo_ejecucion_diarizacion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfxdrma-_MNG",
        "outputId": "ebd7d04d-f001-427d-a9f6-70c6fb38b474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[175.1899130344391, 171.5346760749817]\n"
          ]
        }
      ],
      "source": [
        "print(tiempos_ejecucion_transcripcion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDO_5Gdq_PcV",
        "outputId": "0fcf4ee3-7361-4d6d-8705-7876cf4e5e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.33244811320754725, 0.33244811320754725]\n"
          ]
        }
      ],
      "source": [
        "print(confianzas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "q4lw77tP_Rpi",
        "outputId": "e31a11d5-2502-4247-bbeb-829dcc95820d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ed1613a4-5d33-4791-84e3-b8bf4c89af61\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ed1613a4-5d33-4791-84e3-b8bf4c89af61\")) {                    Plotly.newPlot(                        \"ed1613a4-5d33-4791-84e3-b8bf4c89af61\",                        [{\"marker\":{\"color\":\"indianred\"},\"name\":\"Tiempo de Ejecuci\\u00f3n Transcripci\\u00f3n\",\"x\":[\"FLAC\",\"WAV\",\"WAV49\"],\"y\":[179.8800847530365,169.21809124946594,183.18469500541687],\"type\":\"bar\"},{\"marker\":{\"color\":\"blue\"},\"name\":\"Confianza\",\"x\":[\"FLAC\",\"WAV\",\"WAV49\"],\"y\":[0.33244811320754725,0.33244811320754725,0.33244811320754725],\"yaxis\":\"y2\",\"type\":\"scatter\"},{\"marker\":{\"color\":\"green\"},\"mode\":\"lines+markers\",\"name\":\"Tiempo de Diarizaci\\u00f3n\",\"x\":[\"FLAC\",\"WAV\",\"WAV49\"],\"y\":[237.9792275428772,62.86481809616089,244.7913806438446],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Tiempo de Ejecuci\\u00f3n (s)\"},\"side\":\"left\",\"range\":[0,344.7913806438446]},\"yaxis2\":{\"title\":{\"text\":\"Confianza\"},\"overlaying\":\"y\",\"side\":\"right\",\"range\":[0.23244811320754724,0.4324481132075473],\"tickformat\":\".0%\"},\"legend\":{\"x\":0.01,\"y\":0.99,\"bordercolor\":\"Black\",\"borderwidth\":1},\"title\":{\"text\":\"Tiempo de Ejecuci\\u00f3n y Confianza por Modelo de Transcripci\\u00f3n\"},\"xaxis\":{\"title\":{\"text\":\"Modelo\"}},\"plot_bgcolor\":\"floralwhite\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ed1613a4-5d33-4791-84e3-b8bf4c89af61');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "tiempos_diarizacion=[237.9792275428772, 62.86481809616089,244.7913806438446]\n",
        "confianza = [0.33244811320754725,0.33244811320754725,0.33244811320754725]\n",
        "tiempos_ejecucion_transcripcion = [179.8800847530365,169.21809124946594,183.18469500541687]  # Tiempos de diarización variables\n",
        "modelos = ['FLAC', 'WAV','WAV49']  # Etiquetas para cada modelo\n",
        "\n",
        "# Crear figura de Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Añadir traza para los tiempos de ejecución de la transcripción\n",
        "fig.add_trace(go.Bar(\n",
        "    x=modelos,\n",
        "    y=tiempos_ejecucion_transcripcion,\n",
        "    name='Tiempo de Ejecución Transcripción',\n",
        "    marker_color='indianred'\n",
        "))\n",
        "\n",
        "# Añadir traza para la confianza\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=modelos,\n",
        "    y=confianza,\n",
        "    name='Confianza',\n",
        "    marker_color='blue',\n",
        "    yaxis='y2'\n",
        "))\n",
        "\n",
        "# Añadir traza para los tiempos de diarización\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=modelos,\n",
        "    y=tiempos_diarizacion,\n",
        "    name='Tiempo de Diarización',\n",
        "    marker_color='green',\n",
        "    mode='lines+markers'  # Usando líneas y marcadores para destacar los puntos\n",
        "))\n",
        "\n",
        "# Configuración de los ejes\n",
        "fig.update_layout(\n",
        "    title='Tiempo de Ejecución y Confianza por Modelo de Transcripción',\n",
        "    xaxis=dict(title='Modelo'),\n",
        "    yaxis=dict(title='Tiempo de Ejecución (s)', side='left', range=[0, max(tiempos_ejecucion_transcripcion + tiempos_diarizacion) + 100]),\n",
        "    yaxis2=dict(title='Confianza', overlaying='y', side='right', range=[min(confianza) - 0.1, max(confianza) + 0.1], tickformat=\".0%\"),\n",
        "    legend=dict(x=0.01, y=0.99, bordercolor='Black', borderwidth=1),\n",
        "    plot_bgcolor='floralwhite'\n",
        ")\n",
        "\n",
        "# Mostrar figura\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdFcSxPtR_E_"
      },
      "source": [
        "### Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsDQ1CGjRp3U"
      },
      "source": [
        "\n",
        "Usar el formato WAV sin compresion, ya que el tiempo de ejecucion de la diarizacion se reduce de manera significativa en comparacion a los otros formatos y codecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNbmzR0mS2zK"
      },
      "source": [
        "## Funcion final con las conclusiones dadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jcAhvIfTAI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import datetime\n",
        "import whisper_timestamped as whisper\n",
        "import time\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxiVJX69UQ_R"
      },
      "outputs": [],
      "source": [
        "# Inicializar el modelo de diarización de pyannote\n",
        "diarization_pipeline = Pipeline.from_pretrained(\n",
        "\"pyannote/speaker-diarization-3.1\",\n",
        "use_auth_token=\"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\")\n",
        "diarization_pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "model = whisper.load_model(\"large-v3\", device=device)\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTuADWWwS_yE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def conversion_diarizacion_transcripcion(input_file,model,nombre_salida):\n",
        "  conversiones = [\n",
        "      (\"output_wav_pcm.wav\", \"pcm_s16le\", None)       # WAV sin compresión\n",
        "  ]\n",
        "\n",
        "\n",
        "\n",
        "  for output_file, codec, bitrate in conversiones:\n",
        "      if bitrate:\n",
        "          # Comando para conversiones con bitrate específico\n",
        "          command = f\"ffmpeg -i {input_file} -acodec {codec} -ar 44100 -b:a {bitrate} {output_file}\"\n",
        "      else:\n",
        "          # Comando para conversiones sin bitrate específico (sin pérdida)\n",
        "          command = f\"ffmpeg -i {input_file} -acodec {codec} -ar 44100 {output_file}\"\n",
        "      os.system(command)\n",
        "      print(f\"Archivo {output_file} creado.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Ejecutar diarización\n",
        "  inicio_diarizacion = time.time()\n",
        "\n",
        "  diarization = diarization_pipeline(\"output_wav_pcm.wav\", num_speakers=2)\n",
        "  final_diarizacion = time.time()\n",
        "\n",
        "  tiempo_diarizacion=final_diarizacion-inicio_diarizacion\n",
        "\n",
        "\n",
        "  # Cargar y configurar el modelo de Whisper para el archivo completo\n",
        "  inicio_transcripcion = time.time()\n",
        "\n",
        "  audio = whisper.load_audio(\"output_wav_pcm.wav\")\n",
        "  transcription_result = whisper.transcribe(model, \"output_wav_pcm.wav\", language=\"es\", vad=True,detect_disfluencies=True,\n",
        "                                               beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))\n",
        "  final_transcripcion = time.time()\n",
        "\n",
        "  tiempo_transcripcion=final_transcripcion-inicio_transcripcion\n",
        "\n",
        "    # Preparar el archivo de salida y procesar la diarización y transcripción\n",
        "  with open(f\"{nombre_salida}.txt\", \"w\", encoding='utf-8') as output_file:\n",
        "        last_speaker = None\n",
        "        last_start = 0\n",
        "        last_end = 0\n",
        "        last_transcription = \"\"\n",
        "        total_confidence = 0\n",
        "        segments = transcription_result['segments']\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            transcript_text = segment['text'].strip()\n",
        "            segment_confidence = segment['confidence']\n",
        "            total_confidence += segment_confidence\n",
        "\n",
        "            # Buscar el hablante que más se solapa con este segmento\n",
        "            speaker_label = None\n",
        "            max_overlap = 0\n",
        "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "                overlap = min(end_time, turn.end) - max(start_time, turn.start)\n",
        "                if overlap > max_overlap:\n",
        "                    max_overlap = overlap\n",
        "                    speaker_label = speaker\n",
        "\n",
        "            # Si no hay hablante identificado y estamos en el primer o último segmento, asignamos el último o el primer hablante conocido\n",
        "            if not speaker_label:\n",
        "                if i == 0:\n",
        "                    speaker_label = last_speaker  # Use the last known speaker for the first segment if none identified\n",
        "                elif i == len(segments) - 1:\n",
        "                    speaker_label = last_speaker  # Use the last known speaker for the last segment if none identified\n",
        "\n",
        "            # Combinar segmentos si el hablante es el mismo y no hay pausa significativa\n",
        "            if speaker_label == last_speaker and (start_time - last_end) < 1:\n",
        "                last_end = end_time\n",
        "                last_transcription += \" \" + transcript_text\n",
        "            else:\n",
        "                if last_speaker is not None:\n",
        "                    # Escribir el segmento previo\n",
        "                    output_file.write(f\"{format_time(last_start)} - {format_time(last_end)} [{last_speaker}]: {last_transcription}\\n\")\n",
        "                last_speaker = speaker_label\n",
        "                last_start = start_time\n",
        "                last_end = end_time\n",
        "                last_transcription = transcript_text\n",
        "        # Calcular y mostrar la confianza promedio por segmento para toda la transcripción\n",
        "        average_confidence = total_confidence /  len(transcription_result['segments']) if transcription_result['segments'] else 0\n",
        "        print(f\"Diarization and transcription completed. Average segment confidence: {average_confidence:.2f}\")\n",
        "  return(average_confidence,tiempo_diarizacion,tiempo_transcripcion)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
