{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeXgStKvCGGx",
        "outputId": "f5320a28-007d-4175-8a2e-3c98551bc535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: audplot\n",
            "Successfully installed audplot-1.4.7\n"
          ]
        }
      ],
      "source": [
        "# Librerias necesarias para el correcto funcionamiento\n",
        "!pip install audonnx\n",
        "!pip install audinterface\n",
        "!pip install audb\n",
        "!pip install audmetric\n",
        "!pip install opensmile\n",
        "!pip install audplot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import audeer\n",
        "import audonnx\n",
        "import numpy as np\n",
        "import audinterface\n",
        "import audb\n",
        "import audformat\n",
        "import audmetric\n",
        "import pandas as pd\n",
        "import opensmile\n",
        "import audiofile\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Function to download and extract the model\n",
        "def download_and_extract_model(model_url, dst_path, model_root, cache_root):\n",
        "\n",
        "    if not os.path.exists(dst_path):\n",
        "        # Create cache_root folder if it doesn't exist\n",
        "        os.makedirs(cache_root, exist_ok=True)\n",
        "\n",
        "        audeer.download_url(\n",
        "            model_url,\n",
        "            dst_path,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "    if not os.path.exists(model_root):\n",
        "        # Create model_root folder if it doesn't exist\n",
        "        os.makedirs(model_root, exist_ok=True)\n",
        "\n",
        "        audeer.extract_archive(\n",
        "            dst_path,\n",
        "            model_root,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(model_root):\n",
        "    model = audonnx.load(model_root)\n",
        "    return model\n",
        "\n",
        "# Function to load the Emo-DB database\n",
        "def load_emodb_database(cache_root):\n",
        "    db = audb.load(\n",
        "        'emodb',\n",
        "        version='1.1.1',\n",
        "        format='wav',\n",
        "        mixdown=True,\n",
        "        sampling_rate=16000,\n",
        "        full_path=False,\n",
        "        cache_root=cache_root,\n",
        "        verbose=True,\n",
        "    )\n",
        "    return db\n",
        "\n",
        "def smile_pretrain(emotion, db, cache_root):\n",
        "\n",
        "    smile = opensmile.Smile(\n",
        "        opensmile.FeatureSet.ComParE_2016,\n",
        "        opensmile.FeatureLevel.Functionals,\n",
        "        sampling_rate=16000,\n",
        "        resample=True,\n",
        "        num_workers=5,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    features_smile = smile.process_index(\n",
        "        emotion.index,\n",
        "        root=db.root,\n",
        "        cache_root=audeer.path(cache_root, 'smile'),\n",
        "    )\n",
        "\n",
        "    return features_smile\n",
        "\n",
        "# Function to perform leave-one-speaker-out cross-validation experiment\n",
        "def leave_one_speaker_out_experiment(features, targets, groups, clf):\n",
        "    truths = []\n",
        "    preds = []\n",
        "\n",
        "    logo = LeaveOneGroupOut()\n",
        "\n",
        "    pbar = audeer.progress_bar(\n",
        "        total=len(groups.unique()),\n",
        "        desc='Run experiment',\n",
        "    )\n",
        "    for train_index, test_index in logo.split(\n",
        "        features,\n",
        "        targets,\n",
        "        groups=groups,\n",
        "    ):\n",
        "        train_x = features.iloc[train_index]\n",
        "        train_y = targets[train_index]\n",
        "        clf.fit(train_x, train_y)\n",
        "\n",
        "        truth_x = features.iloc[test_index]\n",
        "        truth_y = targets[test_index]\n",
        "        predict_y = clf.predict(truth_x)\n",
        "\n",
        "        truths.append(truth_y)\n",
        "        preds.append(predict_y)\n",
        "\n",
        "        pbar.update()\n",
        "\n",
        "    truth = pd.concat(truths)\n",
        "    truth.name = 'truth'\n",
        "    pred = pd.Series(\n",
        "        np.concatenate(preds),\n",
        "        index=truth.index,\n",
        "        name='prediction',\n",
        "    )\n",
        "\n",
        "    return truth, pred\n",
        "\n",
        "# Function to process audio features using OpenSMILE\n",
        "def process_features(audio:str):\n",
        "    signal, sampling_rate = audiofile.read(audio)\n",
        "\n",
        "    smile = opensmile.Smile(\n",
        "        opensmile.FeatureSet.ComParE_2016,\n",
        "        opensmile.FeatureLevel.Functionals,\n",
        "        sampling_rate=16000,\n",
        "        resample=True,\n",
        "        num_workers=5,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    features = smile.process_signal(signal, sampling_rate)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to predict emotion from audio features\n",
        "def predict_emotion(features, clf):\n",
        "    predicted_emotion = clf.predict(features)\n",
        "    return predicted_emotion\n",
        "\n",
        "def main():\n",
        "    def cache_path(file):\n",
        "        return os.path.join(cache_root, file)\n",
        "\n",
        "    model_root = 'model'\n",
        "    cache_root = 'cache'\n",
        "\n",
        "    dst_path = cache_path('model.zip')\n",
        "    model_url = 'https://zenodo.org/record/6221127/files/w2v2-L-robust-12.6bc4a7fd-1.1.0.zip'\n",
        "\n",
        "    download_and_extract_model(model_url, dst_path, model_root, cache_root)\n",
        "    model = load_model(model_root)\n",
        "    db = load_emodb_database(cache_root)\n",
        "\n",
        "    speaker = db['files']['speaker'].get()\n",
        "    emotion = db['emotion']['emotion'].get()\n",
        "\n",
        "    audformat.utils.concat([emotion, speaker])\n",
        "\n",
        "    clf = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        SVC(gamma='auto'),\n",
        "    )\n",
        "\n",
        "    features_smile = smile_pretrain(emotion, db, cache_root)\n",
        "\n",
        "    truth_smile, pred_smile = leave_one_speaker_out_experiment(\n",
        "        features_smile,\n",
        "        emotion,\n",
        "        speaker,\n",
        "        clf\n",
        "    )\n",
        "\n",
        "    audmetric.unweighted_average_recall(truth_smile, pred_smile)\n",
        "\n",
        "\n",
        "    base = os.path.abspath('')\n",
        "    file_paths = glob(f\"{base}/*.WAV\")\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        command = f\"ffmpeg -y -i {file_path} -acodec pcm_s16le -ac 1 -ar 16000 {file_path}\"\n",
        "        os.system(command)\n",
        "        print(f\"Archivo {file_path} creado.\")\n",
        "        features = process_features(file_path)\n",
        "        predicted_emotion = predict_emotion(features, clf)\n",
        "        print(\"Predicted Emotion:\", predicted_emotion)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    print(\"End\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyjYqcjrCMCE",
        "outputId": "df7859a3-b2bd-44fd-b745-c92790880db6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:   emodb v1.1.1\n",
            "Cache: /content/cache/emodb/1.1.1/fe182b91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo /content/out-3005017930-900-20240401-100535-1711983935.150382.WAV creado.\n",
            "Predicted Emotion: ['anger']\n",
            "End\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMABDUZWX5IH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}