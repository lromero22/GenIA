{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4FzvL248e9J"
      },
      "outputs": [],
      "source": [
        "!pip install pyannote.audio\n",
        "!pip install pydub\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFuIGWyu7MXU",
        "outputId": "dca33941-58d4-4028-bfdb-2d48defe2dc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        }
      ],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import numpy as np\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tg17_hRe7WpE"
      },
      "outputs": [],
      "source": [
        "# OS BASED PATH\n",
        "# base = Path(__file__).resolve().parent\n",
        "# Notebook Path\n",
        "base = os.path.abspath('')\n",
        "# /content/14838701_2.WAV\n",
        "audio_path = os.path.join(base, \"14838701_2.WAV\")\n",
        "transcribe_path = os.path.join(base, \"transcribe.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZN1UoXVP84tl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def read(k):\n",
        "    y = np.array(k.get_array_of_samples())\n",
        "    return np.float32(y) / 32768\n",
        "\n",
        "def millisec(timeStr):\n",
        "    spl = timeStr.split(\":\")\n",
        "    return (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2])) * 1000)\n",
        "\n",
        "def extract_time_durations(text):\n",
        "    pattern = r'\\[\\s*(\\d{2}:\\d{2}:\\d{2}.\\d{3})\\s*-->\\s*(\\d{2}:\\d{2}:\\d{2}.\\d{3})\\] [a-zA-Z]+\\s([^\\n]+)'\n",
        "    return re.findall(pattern, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8f_tHls7hai",
        "outputId": "fa635038-aa8f-4eb7-a8fb-8c43a781f153"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pipeline = Pipeline.from_pretrained(\n",
        "#     \"pyannote/speaker-diarization\", use_auth_token=\"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\")\n",
        "# pipeline = Pipeline.from_pretrained(\"pyannote/segmentation\", use_auth_token=\"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\")\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\",\n",
        "    use_auth_token=\"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\")\n",
        "\n",
        "k = str(pipeline(str(audio_path), num_speakers=2)).split('\\n')\n",
        "\n",
        "del pipeline\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FXa6hjb17I19"
      },
      "outputs": [],
      "source": [
        "audio = AudioSegment.from_wav(audio_path)\n",
        "audio = audio.set_frame_rate(16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7LIvqj1_ph9",
        "outputId": "8d7f006e-881c-4db6-e248-c9b7ffb753c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [01:59<00:00, 25.8MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"large-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mM8il9Nh7sFS"
      },
      "outputs": [],
      "source": [
        "audio_wh = whisper.load_audio(audio_path)\n",
        "audio_wh = whisper.pad_or_trim(audio_wh)\n",
        "\n",
        "mel = whisper.log_mel_spectrogram(audio=audio_wh, n_mels=128).to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87YKJDzR79vQ",
        "outputId": "0aedf8c0-a709-4248-83c6-38f78426d8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¿Aló? Aló, buenas tardes. Me vuelvo a hablar con el señor Alejandro. ¿Y con él habla? Con Natalia Montoya.\n"
          ]
        }
      ],
      "source": [
        "options = whisper.DecodingOptions(language=\"es\", without_timestamps=True, fp16 = False)\n",
        "result = whisper.decode(model, mel, options)\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8frU1jQ6QjMI",
        "outputId": "8896bf17-8b30-4191-8138-2b345c1e673b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[ 00:01:21.095 -->  00:01:21.723] AD SPEAKER_00'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k[29]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uZdusIms7mW6"
      },
      "outputs": [],
      "source": [
        "for l in range(len(k)):\n",
        "    # Read time from speaker-diarization\n",
        "    j = extract_time_durations(k[l])\n",
        "    j = j[0]\n",
        "    # Convert time to milliseconds\n",
        "    start = int(millisec(j[0]))\n",
        "    end = int(millisec(j[1]))\n",
        "\n",
        "    # Make a trim of the audio\n",
        "    tr = read(audio[start:end])\n",
        "\n",
        "    # Transcribe the chunk of audio\n",
        "    result = model.transcribe(tr, fp16=False, language=\"es\")\n",
        "\n",
        "    # Save it to a file\n",
        "    f = open(transcribe_path, \"a\")\n",
        "    f.write(f'\\n[ {j[0]} -- {j[1]} ] {j[2]} : {result[\"text\"]}')\n",
        "    f.close()\n",
        "\n",
        "    del f\n",
        "    del result\n",
        "    del tr\n",
        "    del j\n",
        "    gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
